{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import normalize\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_table(conf_matrix):\n",
    "    \"\"\"\n",
    "    Given a confusion matrix, return a dataframe with false positives, false negatives and overall error in percentage.\n",
    "    \"\"\" \n",
    "    false_positive = []\n",
    "    false_negative = []\n",
    "    error = []\n",
    "    \n",
    "    for conf_matrix_i in conf_matrix:\n",
    "        tn, fp, fn, tp = conf_matrix_i.ravel()\n",
    "        false_positive.append(fp / np.sum(conf_matrix_i) * 100)\n",
    "        false_negative.append(fn / np.sum(conf_matrix_i) * 100)\n",
    "        error.append((fp+fn) / np.sum(conf_matrix_i) * 100)\n",
    "\n",
    "    results = {\n",
    "        'False positive (%)' : false_positive,\n",
    "        'False negative (%)' : false_negative,\n",
    "        'Overall error (%)' : error\n",
    "    }\n",
    "    \n",
    "    index = ['Fold ' + str(i + 1) for i in range(len(error))]\n",
    "    df = pd.DataFrame(results, index=index)\n",
    "    df.loc['Mean'] = df.mean()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def train_test_clf(x, y, clf, n_folds=7):\n",
    "    \"\"\"\n",
    "    Given a classifier clf, features x and target values y, it trains and predict a model using n_fold cross validation.\n",
    "    \"\"\"\n",
    "    conf_matrix = []\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True)\n",
    "    \n",
    "    for train_index, test_index in kf.split(x):\n",
    "        x_train, x_test = x[train_index], x[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        clf.fit(x_train, y_train)\n",
    "        y_pred = clf.predict(x_test)\n",
    "        confusion_matrix_i = confusion_matrix(y_test, y_pred)\n",
    "        conf_matrix.append(confusion_matrix_i) \n",
    "        \n",
    "    return conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "This notebook analyses spambase data set from [UCI respository](http://archive.ics.uci.edu/ml/datasets/spambase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, data is loaded with pandas and we can see that there are 57 features with last column corresponding to the target value (1 is spam, 0 no spam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2    3     4     5     6     7     8     9  ...    48     49  \\\n",
       "0  0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.00  0.00 ...  0.00  0.000   \n",
       "1  0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94 ...  0.00  0.132   \n",
       "2  0.06  0.00  0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25 ...  0.01  0.143   \n",
       "3  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63 ...  0.00  0.137   \n",
       "4  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63 ...  0.00  0.135   \n",
       "\n",
       "    50     51     52     53     54   55    56  57  \n",
       "0  0.0  0.778  0.000  0.000  3.756   61   278   1  \n",
       "1  0.0  0.372  0.180  0.048  5.114  101  1028   1  \n",
       "2  0.0  0.276  0.184  0.010  9.821  485  2259   1  \n",
       "3  0.0  0.137  0.000  0.000  3.537   40   191   1  \n",
       "4  0.0  0.135  0.000  0.000  3.537   40   191   1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('spambase.data', header=None)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that all data types are numerical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float64    55\n",
       "int64       3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there are no missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df.isna().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we observe that values are not normalized, there is a huge difference in standard deviation and maximum values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.104553</td>\n",
       "      <td>0.213015</td>\n",
       "      <td>0.280656</td>\n",
       "      <td>0.065425</td>\n",
       "      <td>0.312223</td>\n",
       "      <td>0.095901</td>\n",
       "      <td>0.114208</td>\n",
       "      <td>0.105295</td>\n",
       "      <td>0.090067</td>\n",
       "      <td>0.239413</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038575</td>\n",
       "      <td>0.139030</td>\n",
       "      <td>0.016976</td>\n",
       "      <td>0.269071</td>\n",
       "      <td>0.075811</td>\n",
       "      <td>0.044238</td>\n",
       "      <td>5.191515</td>\n",
       "      <td>52.172789</td>\n",
       "      <td>283.289285</td>\n",
       "      <td>0.394045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.305358</td>\n",
       "      <td>1.290575</td>\n",
       "      <td>0.504143</td>\n",
       "      <td>1.395151</td>\n",
       "      <td>0.672513</td>\n",
       "      <td>0.273824</td>\n",
       "      <td>0.391441</td>\n",
       "      <td>0.401071</td>\n",
       "      <td>0.278616</td>\n",
       "      <td>0.644755</td>\n",
       "      <td>...</td>\n",
       "      <td>0.243471</td>\n",
       "      <td>0.270355</td>\n",
       "      <td>0.109394</td>\n",
       "      <td>0.815672</td>\n",
       "      <td>0.245882</td>\n",
       "      <td>0.429342</td>\n",
       "      <td>31.729449</td>\n",
       "      <td>194.891310</td>\n",
       "      <td>606.347851</td>\n",
       "      <td>0.488698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.588000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.276000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.188000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.052000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.706000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>266.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.540000</td>\n",
       "      <td>14.280000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>42.810000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.880000</td>\n",
       "      <td>7.270000</td>\n",
       "      <td>11.110000</td>\n",
       "      <td>5.260000</td>\n",
       "      <td>18.180000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.385000</td>\n",
       "      <td>9.752000</td>\n",
       "      <td>4.081000</td>\n",
       "      <td>32.478000</td>\n",
       "      <td>6.003000</td>\n",
       "      <td>19.829000</td>\n",
       "      <td>1102.500000</td>\n",
       "      <td>9989.000000</td>\n",
       "      <td>15841.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0            1            2            3            4   \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean      0.104553     0.213015     0.280656     0.065425     0.312223   \n",
       "std       0.305358     1.290575     0.504143     1.395151     0.672513   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.420000     0.000000     0.380000   \n",
       "max       4.540000    14.280000     5.100000    42.810000    10.000000   \n",
       "\n",
       "                5            6            7            8            9   \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean      0.095901     0.114208     0.105295     0.090067     0.239413   \n",
       "std       0.273824     0.391441     0.401071     0.278616     0.644755   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.160000   \n",
       "max       5.880000     7.270000    11.110000     5.260000    18.180000   \n",
       "\n",
       "          ...                48           49           50           51  \\\n",
       "count     ...       4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean      ...          0.038575     0.139030     0.016976     0.269071   \n",
       "std       ...          0.243471     0.270355     0.109394     0.815672   \n",
       "min       ...          0.000000     0.000000     0.000000     0.000000   \n",
       "25%       ...          0.000000     0.000000     0.000000     0.000000   \n",
       "50%       ...          0.000000     0.065000     0.000000     0.000000   \n",
       "75%       ...          0.000000     0.188000     0.000000     0.315000   \n",
       "max       ...          4.385000     9.752000     4.081000    32.478000   \n",
       "\n",
       "                52           53           54           55            56  \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000   4601.000000   \n",
       "mean      0.075811     0.044238     5.191515    52.172789    283.289285   \n",
       "std       0.245882     0.429342    31.729449   194.891310    606.347851   \n",
       "min       0.000000     0.000000     1.000000     1.000000      1.000000   \n",
       "25%       0.000000     0.000000     1.588000     6.000000     35.000000   \n",
       "50%       0.000000     0.000000     2.276000    15.000000     95.000000   \n",
       "75%       0.052000     0.000000     3.706000    43.000000    266.000000   \n",
       "max       6.003000    19.829000  1102.500000  9989.000000  15841.000000   \n",
       "\n",
       "                57  \n",
       "count  4601.000000  \n",
       "mean      0.394045  \n",
       "std       0.488698  \n",
       "min       0.000000  \n",
       "25%       0.000000  \n",
       "50%       0.000000  \n",
       "75%       1.000000  \n",
       "max       1.000000  \n",
       "\n",
       "[8 rows x 58 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not important for models like tree ensembles but it will affect other models as SVM. We normalize x data therefore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(df.iloc[:, :-1])\n",
    "y = np.array(df.iloc[:,-1])\n",
    "x = normalize(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, once we have already data preprocessed, it is time to choose our machine learning models for classification.\n",
    "If there is no more knowledge about what is the most suitable model and I had to pick only one, I will go directly with XGBoost or any other boosting method based on decision trees.\n",
    "In this notebook five different models have been compared:\n",
    "- XGBoost\n",
    "- Naive Bayes\n",
    "- Random Forest\n",
    "- Support Vector Machine\n",
    "- KNearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    'XGBoost' : XGBClassifier(),\n",
    "    'Naive Bayes' : GaussianNB(),\n",
    "    'Random Forest' : RandomForestClassifier(n_estimators=5),\n",
    "    'SVM' : SVC(gamma='scale'),\n",
    "    'KNN' : KNeighborsClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every model we train it with 7 fold cross validation and store the result of the test prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for clf_name, clf in classifiers.items():\n",
    "    conf_matrix = train_test_clf(x, y, clf, n_folds=7)\n",
    "    results[clf_name] = build_table(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to compare which performs better and we can see that XGBoost is the best model in this case with an overall error of around 5-6%.\n",
    "This are only preliminar results, as we have not tuned the hyperparameters in models which affect greatly the result. Some examples of this parameters are gamma in the case of SVM or the metric distance for KNN. \n",
    "This is one of the advantages of Boosting methods and trees as there are not many parameters to adjast and the model performs well in a wide range of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a63bd3ab70>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "comparison = []\n",
    "for res in results.values():\n",
    "    comparison.append(res.loc['Mean'])\n",
    "comp = pd.DataFrame(comparison, index=classifiers.keys())\n",
    "comp.sort_values('Overall error (%)').iloc[:, :-1].plot.bar(stacked=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us analyze in more detail the results of our model XGBoost showing a table with the results of the 7 folds and the average result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>False positive (%)</th>\n",
       "      <th>False negative (%)</th>\n",
       "      <th>Overall error (%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Fold 1</th>\n",
       "      <td>2.888</td>\n",
       "      <td>4.711</td>\n",
       "      <td>7.599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fold 2</th>\n",
       "      <td>1.976</td>\n",
       "      <td>3.647</td>\n",
       "      <td>5.623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fold 3</th>\n",
       "      <td>2.131</td>\n",
       "      <td>2.588</td>\n",
       "      <td>4.718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fold 4</th>\n",
       "      <td>1.826</td>\n",
       "      <td>4.262</td>\n",
       "      <td>6.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fold 5</th>\n",
       "      <td>2.131</td>\n",
       "      <td>3.196</td>\n",
       "      <td>5.327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fold 6</th>\n",
       "      <td>2.588</td>\n",
       "      <td>3.957</td>\n",
       "      <td>6.545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fold 7</th>\n",
       "      <td>3.196</td>\n",
       "      <td>5.023</td>\n",
       "      <td>8.219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean</th>\n",
       "      <td>2.391</td>\n",
       "      <td>3.912</td>\n",
       "      <td>6.303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        False positive (%)  False negative (%)  Overall error (%)\n",
       "Fold 1               2.888               4.711              7.599\n",
       "Fold 2               1.976               3.647              5.623\n",
       "Fold 3               2.131               2.588              4.718\n",
       "Fold 4               1.826               4.262              6.088\n",
       "Fold 5               2.131               3.196              5.327\n",
       "Fold 6               2.588               3.957              6.545\n",
       "Fold 7               3.196               5.023              8.219\n",
       "Mean                 2.391               3.912              6.303"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = results['XGBoost'].round(3)\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the advantages of tree classifiers is that we can analyze the feature importance. Let us see the highest 10 features by importance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_importance(classifiers['XGBoost'], max_num_features=10 )\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 3 features are 24, 26 and 15, which corresponds to word_freq_hp, word_freq_george and word_freq_free. \n",
    "First two features does not make much sense for me but the third one, free, it is obvious its relation with spam. Fifth feature(f6), char_freq_$ is also a good clue.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "Using XGBoost we have built a model for spam detection with an overall error of 5 to 6%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "We have not tuned the hyperparameters for models, so to really compare their error and to obtain better results, the next step will be to find the optimal of these parameters doing a search with cross validation.\n",
    "Furthermore, here are some ideas:\n",
    "- We can see that boosting yields a model with equal parts of false positives and false negatives. We could try to combine these model and create an ensemble with a model that has a different ratio of these errors.\n",
    "- For example, Naive Bayes shows a small false negative error, which we could use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just for fun\n",
    "I have implemented different machine algorithms from scratch using only numpy library, which can be seen in [my repository](https://github.com/MarinAlejandro/machine-learning). Next lines uses the implementation of Adaboost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import accuracy\n",
    "from boosting import AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost with 20 classifiers accuracy: 0.77\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)\n",
    "n_classifiers = 20\n",
    "clf = AdaBoost(n_classifiers)\n",
    "clf.train(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "accuracy_i = accuracy(y_test, y_pred)\n",
    "print(f'Adaboost with {n_classifiers} classifiers accuracy: {accuracy_i :.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not really impressive, but this predictions are done combining a simple stump!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
